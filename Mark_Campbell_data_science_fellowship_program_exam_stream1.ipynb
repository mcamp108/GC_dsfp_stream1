{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mark Campbell\n",
    "Data Science Fellowship program exam\n",
    "Stream 1 - Machine Learning\n",
    "2022-12-03\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import image\n",
    "\n",
    "import scipy.io\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "projdir = r'C:\\Users\\Mark\\Documents\\Resume\\gc_data_science_fellowship'\n",
    "IMGS_PATH = os.path.join(projdir, 'car_ims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1 - Build a function that converts a labelled dataset into labelled and unlabelled subsets.\n",
    "\n",
    "def labelled_unlabelled_split(dataset_labels, proportion):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    Create a logical mask that can be used to perform a stratified split on the dataset from which the input \n",
    "    dataset_labels come. The logical mask is True for samples that should be labelled (training set) and False for samples that \n",
    "    should be unlabelled (testing set). \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_labels (list):\n",
    "        list of labels. Labels must be integers.\n",
    "    proportion (float):\n",
    "        the proportion of instances for each unique class label in dataset_labels that should be included in the training set.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    labelled_subset_mask (array):\n",
    "        Array of booleans the same length as dataset_labels. An element is True if the label is to be included in the \n",
    "        labelled dataset, False otherwise.\n",
    "    kept_labels (array):\n",
    "        the labels of the samples for which the subset mask is True\n",
    "    unkept_labels (array):\n",
    "        the labels of the samples for which the subset mask is False\n",
    "    \"\"\"\n",
    "    assert((proportion >= 0) & (proportion <= 1)), \"argument 'proportion' must be between 0 and 1, inclusive\"    \n",
    "    labels_ = np.asarray(dataset_labels.copy())    \n",
    "    classes, counts = np.unique(dataset_labels, return_counts=True)\n",
    "    labelled_subset_mask = np.full((sum(counts)), False)\n",
    "    \n",
    "    for c, count in zip(classes, counts):\n",
    "        keep = int(round(count * proportion))\n",
    "        # ensure each class has at least one instance labelled within the dataset\n",
    "        keep = max([1, keep])\n",
    "        if keep == count:\n",
    "            warnings.warn(f'Insufficient samples - class label {c} does not appear in the unlabelled dataset')\n",
    "        \n",
    "        this_class_idx = np.where(labels_ == c)[0]\n",
    "        keep_this_label = this_class_idx[: keep]\n",
    "        # record label was kept\n",
    "        labelled_subset_mask[keep_this_label] = True\n",
    "    # end for\n",
    "    kept_labels = labels_[labelled_subset_mask]\n",
    "    unkept_labels = labels_[~labelled_subset_mask]\n",
    "    \n",
    "    return labelled_subset_mask, kept_labels, unkept_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# TASK 2 - Data cleaning\n",
    "\n",
    "\n",
    "def find_non_rgb_img(path):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    Identify images that are not in RBG format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    path (str):\n",
    "        Path containing images\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        flagged_paths (list):\n",
    "            full paths of images within path that were not in RBG format.\n",
    "    \"\"\"        \n",
    "    flagged_paths = []\n",
    "    for fname in os.listdir(path):\n",
    "        full_path = os.path.join(path, fname)\n",
    "        img = image.imread(full_path)\n",
    "        if img.ndim != 3 or img.shape[2] != 3:\n",
    "            flagged_paths.append(full_path)\n",
    "        # end if\n",
    "    # end for\n",
    "    return flagged_paths\n",
    "    \n",
    "    \n",
    "delete_imgs = find_non_rgb_img(IMGS_PATH)\n",
    "print(len(delete_imgs))\n",
    "\n",
    "for path in delete_imgs:\n",
    "    os.remove(path)\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Mark/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# TASK 3 - Dataset representation\n",
    "\n",
    "\n",
    "def load_annotations(mat_file):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    Load cars dataset annotations from .mat file to pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mat_file (str):\n",
    "        full path to .mat file containing cars dataset annotations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    annotations (pd.DataFrame)\n",
    "        loaded annotations\n",
    "    \"\"\"\n",
    "    loaded = scipy.io.loadmat(mat_file)\n",
    "    annotations = loaded['annotations'][0]\n",
    "    columns = list(annotations.dtype.names)\n",
    "    unpacked_ann = []\n",
    "    for ann in annotations:\n",
    "        unpacked_ann.append([elem.flat[0] for elem in ann])\n",
    "    # end for\n",
    "    annotations = pd.DataFrame(data=unpacked_ann, columns=columns)\n",
    "    try:\n",
    "        fnames_ = annotations['relative_im_path']\n",
    "        fnames = [name.split('/')[-1] for name in fnames_]\n",
    "        annotations['fnames'] = fnames\n",
    "    except:\n",
    "        pass\n",
    "    # end try\n",
    "    return annotations\n",
    "    \n",
    "\n",
    "def mk_data_representation_map(imgs_path, annotations_path, model, preprocess):\n",
    "    representation = {}\n",
    "    annotations = load_annotations(annotations_path)\n",
    "    has_class_column = 'class' in annotations.columns\n",
    "    for i, fname in enumerate(os.listdir(imgs_path)):\n",
    "        path = os.path.join(imgs_path, fname)\n",
    "        embedding = get_embedding(model, preprocess, path)\n",
    "        \n",
    "        row = annotations.loc[annotations.fnames == fname]\n",
    "        has_label = has_class_column\n",
    "        if has_class_column:\n",
    "            class_idx = row['class'].values[0]\n",
    "            if class_idx is None:\n",
    "                has_label = False\n",
    "            # end if\n",
    "        else:\n",
    "            class_idx = None\n",
    "        # end if\n",
    "        representation[i] = {\n",
    "            'embedding': embedding,\n",
    "            'class_idx': class_idx, \n",
    "            'labelled': has_label}\n",
    "    # end for\n",
    "    return representation\n",
    "\n",
    "\n",
    "def get_embedding(model, preprocess, img_path):\n",
    "    input_img = Image.open(img_path)\n",
    "    input_tensor = preprocess(input_img)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_batch)\n",
    "    # end with\n",
    "    return embedding\n",
    "\n",
    "    \n",
    "# {1: {'embedding': <np.ndarray>, 'class_idx': <int>, â€˜labelled': <boolean or int>}\n",
    "    \n",
    "annotations_path = os.path.join(projdir, 'labels', 'cars_annos.mat')\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.fc = torch.nn.Identity()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "all_data = mk_data_representation_map(IMGS_PATH, annotations_path, model, preprocess)\n",
    "    \n",
    "save_path = os.path.join(projdir, 'data.pt')\n",
    "\n",
    "torch.save(all_data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 - Build a partially labelled dataset\n",
    "\n",
    "data = torch.load(save_path)\n",
    "dataset_labels = [entry['class_idx'] for entry in data.values()]\n",
    "labelled_subset_mask, kept_labels, unkept_labels = labelled_unlabelled_split(dataset_labels, 0.4)\n",
    "\n",
    "partial_dataset = {}\n",
    "for key, val in data.items():\n",
    "    new_val = val.copy()\n",
    "    if not labelled_subset_mask[key]:\n",
    "        new_val['class_idx'] = np.nan\n",
    "        new_val['labelled'] = False\n",
    "    # end if\n",
    "    partial_dataset[key] = new_val\n",
    "# end for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5 - Create train/validation split\n",
    "\n",
    "\n",
    "def train_test_split(dataset_inputs, dataset_labels, training_proportion):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    # TASK 5 - Create train/validation split\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_inputs (list):\n",
    "        list of feature vectors or embeddings.    \n",
    "    dataset_labels (list):\n",
    "        The class label corresponding to each element in dataset_inputs.    \n",
    "    training_proportion (float)\n",
    "        The proportion ([0, 1]) of instances for each unique class label in dataset_labels that should be included in the \n",
    "        training set.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    training_inputs (array):\n",
    "        Elements from dataset_inputs that comprise the training set.\n",
    "    training_labels (array):\n",
    "        The class labels for\n",
    "    test_inputs (array):\n",
    "        Elements from dataset_inputs that comprise the testing set.\n",
    "    test_labels (array):\n",
    "        The class labels for test_inputs.\n",
    "    \"\"\"    \n",
    "    labelled_subset_mask, training_labels, test_labels = labelled_unlabelled_split(dataset_labels, training_proportion)\n",
    "    training_inputs = dataset_inputs[labelled_subset_mask]\n",
    "    test_inputs = dataset_inputs[~labelled_subset_mask]\n",
    "    return training_inputs, training_labels, test_inputs, test_labels \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6 - Create experiment(s) to convince clients that more labelled data will improve model performance\n",
    "\n",
    "\n",
    "`To convince clients to label some more data, I will create a demonstration that shows how increasing the number of labels \n",
    "increases model prediction accuracy. To accomplish this, I will divide the labelled dataset into two subsets. The first subset \n",
    "will contain 75% of the labelled data (30% of the full dataset) used for training. The second subset will contain the remaining \n",
    "25% of the labelled data (10% of the full dataset) used for testing. Models will be trained using 10, 20, 30, 40, 50,â€¦ 100% of \n",
    "the labelled data set aside for training. Each model will then be evaluated on the same testing set, which is comprised of the \n",
    "same 25% of the labelled data. By plotting accuracy as a function of training set size, I will be able to estimate the impact \n",
    "of additional labelled data on model accuracy. Then, the clients will be able to judge if the increase in model accuracy \n",
    "expected from the additional data labels is worth the cost of labelling additional data.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "partial_labels = np.array([entry['class_idx'] for entry in partial_dataset.values() if entry['labelled']])\n",
    "partial_inputs = [np.array(entry['embedding']) for entry in partial_dataset.values() if entry['labelled']]\n",
    "partial_inputs = np.concatenate(partial_inputs, axis=0)\n",
    "props = np.arange(0.1, 1.1, 0.1)\n",
    "accuracies = []\n",
    "for prop in props:\n",
    "    train_x, train_y, test_x, test_y = train_test_split(partial_inputs, partial_labels, prop)\n",
    "    model = SGDClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    score = model.score(test_x, test_y)\n",
    "    accuracies.append(score)\n",
    "# end for\n",
    "fg, ax = plt.subplots(1, 1, figsize=(16,9))\n",
    "ax.plot(props, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7 - Active learning to select new instances to be labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 8 - Final model training and evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
